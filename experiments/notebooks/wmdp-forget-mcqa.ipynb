{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15dba3c",
   "metadata": {},
   "source": [
    "# Download and Prepare Datasets\n",
    "\n",
    "Downloads and prepares datasets for WMDP bio unlearning experiments:\n",
    "\n",
    "**Datasets:**\n",
    "- Gibberish: Questions rewritten with nonsense words\n",
    "- Real Words Sciency: Questions rewritten with scientific-sounding words\n",
    "- Nonsensical Biology: Questions rewritten with biology jargon\n",
    "- WMDP Bio Robust: Original WMDP biology questions (6 subtopics combined)\n",
    "\n",
    "Filters nonsense datasets to match WMDP questions and saves as CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723805a7",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed84e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/nk1924/home/ign-inf-unlearning/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading nonsense datasets from HuggingFace...\n",
      "\n",
      "üì• Loading WMDP Bio Robust dataset (6 configs)...\n",
      "\n",
      "‚úÖ All datasets loaded successfully!\n",
      "\n",
      "   Gibberish: 1243 questions\n",
      "   Real Words Sciency: 1250 questions\n",
      "   Nonsensical Biology: 1244 questions\n",
      "   WMDP Bio: 868 questions (6 configs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup paths and authentication\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / \"paths.py\").exists() and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from paths import ENV_FILE, DATASETS_PATH\n",
    "\n",
    "load_dotenv(ENV_FILE)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"‚ùå HF_TOKEN not found in .env file\")\n",
    "\n",
    "# Load nonsense datasets\n",
    "print(\"üì• Loading nonsense datasets from HuggingFace...\\n\")\n",
    "gibberish_dataset = load_dataset(\"NLie2/rewrite-questions-gibberish\", token=hf_token)\n",
    "real_words_sciency_dataset = load_dataset(\"NLie2/rewrite-questions-real-words-sciency\", token=hf_token)\n",
    "nonsensical_biology_dataset = load_dataset(\"NLie2/rewrite-questions-nonsensical-biology\", token=hf_token)\n",
    "\n",
    "# Load WMDP bio dataset (all 6 configs)\n",
    "print(\"üì• Loading WMDP Bio Robust dataset (6 configs)...\\n\")\n",
    "wmdp_configs = [\n",
    "    'bioweapons_and_bioterrorism',\n",
    "    'dual_use_virology', \n",
    "    'enhanced_potential_pandemic_pathogens',\n",
    "    'expanding_access_to_threat_vectors',\n",
    "    'reverse_genetics_and_easy_editing',\n",
    "    'viral_vector_research'\n",
    "]\n",
    "\n",
    "wmdp_dfs = []\n",
    "for config in wmdp_configs:\n",
    "    dataset = load_dataset(\"EleutherAI/wmdp_bio_robust_mcqa\", config, token=hf_token)\n",
    "    split = 'robust' if 'robust' in dataset else list(dataset.keys())[0]\n",
    "    df = dataset[split].to_pandas()\n",
    "    df['config'] = config\n",
    "    wmdp_dfs.append(df)\n",
    "\n",
    "wmdp_bio_combined = pd.concat(wmdp_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ All datasets loaded successfully!\\n\")\n",
    "print(f\"   Gibberish: {len(gibberish_dataset['train'])} questions\")\n",
    "print(f\"   Real Words Sciency: {len(real_words_sciency_dataset['train'])} questions\")\n",
    "print(f\"   Nonsensical Biology: {len(nonsensical_biology_dataset['train'])} questions\")\n",
    "print(f\"   WMDP Bio: {len(wmdp_bio_combined)} questions ({len(wmdp_configs)} configs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0c93c",
   "metadata": {},
   "source": [
    "## Filter & Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babe88fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Filtering to questions in ALL datasets:\n",
      "\n",
      "   Questions in all 3 nonsense datasets: 1197\n",
      "   Questions also in WMDP: 811\n",
      "\n",
      "   After filtering:\n",
      "   Gibberish:           1243 ‚Üí 811\n",
      "   Real Words Sciency:  1250 ‚Üí 811\n",
      "   Nonsensical Biology: 1244 ‚Üí 811\n",
      "   WMDP Bio:             868 ‚Üí 811\n",
      "\n",
      "‚úÖ All datasets now have 811 matching questions!\n"
     ]
    }
   ],
   "source": [
    "# Get unique WMDP questions\n",
    "wmdp_questions = set(wmdp_bio_combined['question'].values)\n",
    "\n",
    "# Convert to pandas\n",
    "gibberish_df = gibberish_dataset['train'].to_pandas()\n",
    "real_words_df = real_words_sciency_dataset['train'].to_pandas()\n",
    "nonsensical_df = nonsensical_biology_dataset['train'].to_pandas()\n",
    "\n",
    "# Find questions that exist in ALL three nonsense datasets AND in WMDP\n",
    "gib_questions = set(gibberish_df['original'].unique())\n",
    "real_questions = set(real_words_df['original'].unique())\n",
    "nons_questions = set(nonsensical_df['original'].unique())\n",
    "\n",
    "# Get intersection: questions in all three nonsense datasets\n",
    "common_questions = gib_questions.intersection(real_questions).intersection(nons_questions)\n",
    "\n",
    "# Further filter to only those in WMDP\n",
    "final_questions = common_questions.intersection(wmdp_questions)\n",
    "\n",
    "# Filter all datasets to this common set\n",
    "gibberish_filtered = gibberish_df[gibberish_df['original'].isin(final_questions)].copy()\n",
    "real_words_filtered = real_words_df[real_words_df['original'].isin(final_questions)].copy()\n",
    "nonsensical_filtered = nonsensical_df[nonsensical_df['original'].isin(final_questions)].copy()\n",
    "wmdp_filtered = wmdp_bio_combined[wmdp_bio_combined['question'].isin(final_questions)].copy()\n",
    "\n",
    "# Show filtering results\n",
    "print(f\"üîç Filtering to questions in ALL datasets:\\n\")\n",
    "print(f\"   Questions in all 3 nonsense datasets: {len(common_questions)}\")\n",
    "print(f\"   Questions also in WMDP: {len(final_questions)}\")\n",
    "print(f\"\\n   After filtering:\")\n",
    "print(f\"   Gibberish:           {len(gibberish_df):4d} ‚Üí {len(gibberish_filtered):3d}\")\n",
    "print(f\"   Real Words Sciency:  {len(real_words_df):4d} ‚Üí {len(real_words_filtered):3d}\")\n",
    "print(f\"   Nonsensical Biology: {len(nonsensical_df):4d} ‚Üí {len(nonsensical_filtered):3d}\")\n",
    "print(f\"   WMDP Bio:            {len(wmdp_bio_combined):4d} ‚Üí {len(wmdp_filtered):3d}\")\n",
    "print(f\"\\n‚úÖ All datasets now have {len(final_questions)} matching questions!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7595f7b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74192f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving to: /rds/general/user/nk1924/home/ign-inf-unlearning/data/datasets/input_data/wmdp-bio-forget-mcqa\n",
      "\n",
      "   ‚úì gibberish.csv             (811 questions)\n",
      "   ‚úì real_words_sciency.csv    (811 questions)\n",
      "   ‚úì nonsensical_biology.csv   (811 questions)\n",
      "   ‚úì wmdp_bio_robust.csv       (811 questions)\n",
      "\n",
      "‚úÖ All datasets saved!\n"
     ]
    }
   ],
   "source": [
    "# Create output directory and save\n",
    "output_dir = DATASETS_PATH / \"input_data/wmdp-bio-forget-mcqa\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datasets_to_save = [\n",
    "    (\"gibberish.csv\", gibberish_filtered),\n",
    "    (\"real_words_sciency.csv\", real_words_filtered),\n",
    "    (\"nonsensical_biology.csv\", nonsensical_filtered),\n",
    "    (\"wmdp_bio_robust.csv\", wmdp_filtered)\n",
    "]\n",
    "\n",
    "print(f\"\\nüíæ Saving to: {output_dir}\\n\")\n",
    "for filename, df in datasets_to_save:\n",
    "    path = output_dir / filename\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"   ‚úì {filename:25s} ({len(df):3d} questions)\")\n",
    "\n",
    "print(f\"\\n‚úÖ All datasets saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cff00",
   "metadata": {},
   "source": [
    "# Upload to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb00e78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: gibberish.csv\n",
      "Target repo: NLie2/rewrite-questions-gibberish\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9143d313624c74a15afc04ee213fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c16f9b77754b3d8baa47539005da5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f290aeb9e44898a7375c2344fae748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d95c339b46449786d3efda8d690c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Uploaded 811 records to 'robust' split\n",
      "  View at: https://huggingface.co/datasets/NLie2/rewrite-questions-gibberish\n",
      "\n",
      "Processing: real_words_sciency.csv\n",
      "Target repo: NLie2/rewrite-questions-real-words-sciency\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14b5154dcb0494e87106ae26456f2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449dbac8e05c459d815a6e8e2545abf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641bb595b8e04ab69243962af483ab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1f30902fae42af8097910e42b8b5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Uploaded 811 records to 'robust' split\n",
      "  View at: https://huggingface.co/datasets/NLie2/rewrite-questions-real-words-sciency\n",
      "\n",
      "Processing: nonsensical_biology.csv\n",
      "Target repo: NLie2/rewrite-questions-nonsensical-biology\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb613c49ddd4782a8764d85686734de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eec68e4b5e14b168eb2ed02c457698e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccec79d9caf54d5bae7242484b8a996c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e83d2b6d29245b387cf24b09b622299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f64ca38cdff4305b8d3a4955cf37875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Uploaded 811 records to 'robust' split\n",
      "  View at: https://huggingface.co/datasets/NLie2/rewrite-questions-nonsensical-biology\n",
      "\n",
      "‚úÖ All robust versions uploaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "token = os.getenv('HF_TOKEN')\n",
    "\n",
    "# Configuration - map CSV files to their repos\n",
    "source_dir = Path(\"/rds/general/user/nk1924/home/ign-inf-unlearning/data/datasets/input_data/wmdp-bio-forget-mcqa\")\n",
    "\n",
    "file_to_repo = {\n",
    "    \"gibberish.csv\": \"NLie2/rewrite-questions-gibberish\",\n",
    "    \"real_words_sciency.csv\": \"NLie2/rewrite-questions-real-words-sciency\",\n",
    "    \"nonsensical_biology.csv\": \"NLie2/rewrite-questions-nonsensical-biology\"\n",
    "}\n",
    "\n",
    "# Process and upload each CSV as \"robust\" split\n",
    "for filename, repo_id in file_to_repo.items():\n",
    "    file_path = source_dir / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"‚ö†Ô∏è File not found: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    print(f\"Target repo: {repo_id}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert to HF Dataset and upload as \"robust\" split\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset.push_to_hub(\n",
    "        repo_id=repo_id,\n",
    "        token=token,\n",
    "        split=\"robust\"  # All go to \"robust\" split\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Uploaded {len(df)} records to 'robust' split\")\n",
    "    print(f\"  View at: https://huggingface.co/datasets/{repo_id}\")\n",
    "\n",
    "print(\"\\n‚úÖ All robust versions uploaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
